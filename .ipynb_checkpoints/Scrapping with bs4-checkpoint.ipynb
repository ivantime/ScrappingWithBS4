{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6cd7469c",
   "metadata": {},
   "source": [
    "# Proxy Rotation (Note: Use EliteProxies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7c4f2448",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c72b3048",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trying proxy: https://47.240.160.90:10001\n",
      "Bad proxy...\n",
      "Trying proxy: https://195.158.30.232:3128\n",
      "Bad proxy...\n",
      "Trying proxy: https://5.161.105.105:80\n",
      "Bad proxy...\n",
      "Trying proxy: https://43.225.23.132:80\n",
      "Bad proxy...\n",
      "Trying proxy: https://150.136.108.121:3128\n",
      "Bad proxy...\n",
      "Trying proxy: https://85.25.91.141:15333\n",
      "Bad proxy...\n",
      "Trying proxy: https://85.195.104.71:80\n",
      "Bad proxy...\n",
      "Trying proxy: https://38.108.119.176:59394\n",
      "Bad proxy...\n",
      "Trying proxy: https://50.231.95.3:8080\n",
      "Bad proxy...\n",
      "Trying proxy: https://18.230.58.67:3128\n",
      "Bad proxy...\n",
      "Trying proxy: https://78.46.123.202:80\n",
      "Bad proxy...\n",
      "Trying proxy: https://5.189.184.6:80\n",
      "Bad proxy...\n",
      "Trying proxy: https://1.10.141.220:54620\n",
      "Bad proxy...\n",
      "Trying proxy: https://154.66.210.1:8080\n",
      "Bad proxy...\n",
      "Trying proxy: https://151.106.17.122:1080\n",
      "Bad proxy...\n",
      "Trying proxy: https://151.106.17.125:1080\n",
      "Bad proxy...\n",
      "Trying proxy: https://151.106.17.123:1080\n",
      "Bad proxy...\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'json'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_13184/981177349.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     33\u001b[0m     \u001b[0mres\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfetch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'https://www.google.com/'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     34\u001b[0m     \u001b[0mproxy_index\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 35\u001b[1;33m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'ip'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mres\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjson\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'ip'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     36\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'url:'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mres\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjson\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'url'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'json'"
     ]
    }
   ],
   "source": [
    "res = requests.get('https://free-proxy-list.net')\n",
    "content = BeautifulSoup(res.text, 'lxml')\n",
    "table = content.find('table')\n",
    "rows = table.find_all('tr')\n",
    "cols = [[col.text for col in row.find_all('td')] for row in rows]\n",
    "\n",
    "proxies = []\n",
    "proxy_index = 0\n",
    "\n",
    "for col in cols:\n",
    "    try:\n",
    "        if col[4] == 'elite proxy' and col[6] == 'yes':\n",
    "            proxies.append('https://' + col[0] + ':' + col[1])\n",
    "    except:\n",
    "        pass\n",
    "# print (proxies)\n",
    "\n",
    "def fetch(url, params):\n",
    "    global proxy_index\n",
    "    \n",
    "    while proxy_index < len(proxies):\n",
    "        try:\n",
    "            print('Trying proxy:', proxies[proxy_index])\n",
    "            res = requests.get(url, proxies={'https': proxies[proxy_index]}, params=params, timeout=5)\n",
    "            return res\n",
    "            \n",
    "        except:\n",
    "            print('Bad proxy...')\n",
    "            proxy_index += 1\n",
    "\n",
    "for page in range(0, 4):\n",
    "    params = {'page': page}\n",
    "    res = fetch('https://www.google.com/', params=params)\n",
    "    proxy_index += 1\n",
    "    print('ip', res.json()['ip'])\n",
    "    print('url:', res.json()['url'])  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0242ed9",
   "metadata": {},
   "source": [
    "# ------ OpenLibrary.org (below)------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cee9436",
   "metadata": {},
   "source": [
    "# Word Combinations (of 20 Words Given)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "341ecfb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import time\n",
    "import requests\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6be7aec9",
   "metadata": {},
   "outputs": [],
   "source": [
    "words2Scrap=[\"Table\", \"Fog\", \"Wikipedia\", \"Empire\", \"Ruin\", \"Era\", \"English\", \"Library\", \"Twin\", \"Tower\", \"Book\", \"Art\",\n",
    "             \"Science\", \"Poor\", \"Rich\", \"Dad\", \"Heart\", \"Power\", \"Medicine\", \"Borrow\" ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "68e11ad2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import combinations\n",
    "\n",
    "#Source: https://stackoverflow.com/questions/464864/how-to-get-all-possible-combinations-of-a-list-s-elements#answer-32555776\n",
    "def combs(x):\n",
    "    return [c for i in range(len(x)+1) for c in combinations(x,i)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "622b3728",
   "metadata": {},
   "outputs": [],
   "source": [
    "lister= list(combs(words2Scrap))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "726acc82",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1048576"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(lister)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6fd91a82",
   "metadata": {},
   "outputs": [],
   "source": [
    "def formatSearchTerm4url(typle,index):\n",
    "    searchTerm=\"\"\n",
    "    for i,d in enumerate(list(typle[index])):\n",
    "        if (i!=0):\n",
    "            searchTerm+=\"+\"+str(d)\n",
    "        else:\n",
    "            searchTerm=d\n",
    "    urlMain= f\"https://openlibrary.org/search?q=paper+trains&mode=everything\"\n",
    "    return urlMain\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "2d96250b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getRobotDelay(num_runs,delayTime):\n",
    "    #every 10 runs/pages or if first time, get delay from robots.txt\n",
    "    if (num_runs%20==0 or delayTime==0):\n",
    "        res = requests.get('https://openlibrary.org/robots.txt')\n",
    "        content = BeautifulSoup(res.text, 'lxml')\n",
    "\n",
    "        delayTime = float(str(content).split(\"\\nCrawl-delay: \")[-1].split(\"\\n\")[0])\n",
    "        print(\"just got robot.txt... current delay time: \",delayTime)\n",
    "        return delayTime\n",
    "    else:\n",
    "        return delayTime\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c847287d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "import numpy\n",
    "\n",
    "def getLinksOfPageURL(lister, index,urls, num_runs, delayTime):\n",
    "    delayTime = getRobotDelay(num_runs,delayTime)\n",
    "    \n",
    "    if (type(lister) is not str):\n",
    "        URL =formatSearchTerm4url(lister,index)\n",
    "        page = requests.get(URL)\n",
    "\n",
    "        soup = BeautifulSoup(page.content, \"html.parser\")\n",
    "        job_elements = soup.find(class_=\"pagination\")\n",
    "\n",
    "        page_no=0\n",
    "\n",
    "        if (lister[index]!=()):\n",
    "            pages= job_elements.find_all(class_=\"ChoosePage\",href=True)\n",
    "\n",
    "            print(f\"\\n\\n\\n\\n\\n\\nURL of Index {index}...\")\n",
    "            for indexPage,data in enumerate(pages):\n",
    "                if (indexPage==0):\n",
    "                    #then add page 1 (since there's page 2)\n",
    "                    url=\"https://openlibrary.org\"+data['href'][:len(data['href'])-1]+\"1\"\n",
    "                    page_no=1\n",
    "                    print (\"Found URL:\", (url))\n",
    "                    urls.append(url)\n",
    "                    \n",
    "                    #starts with page 2 thus add this too\n",
    "                    url=\"https://openlibrary.org\"+data['href']\n",
    "                    print (\"Found URL:\",url )\n",
    "                    page_no=int(data.text)\n",
    "                    urls.append(url)\n",
    "                    \n",
    "                else:\n",
    "                    try:\n",
    "                        url=\"https://openlibrary.org\"+data['href']\n",
    "                        print (\"Found URL:\",url )\n",
    "                        page_no=int(data.text)\n",
    "                        urls.append(url)\n",
    "                        \n",
    "                        print(len(urls))\n",
    "                    except:\n",
    "                        pass\n",
    "                    \n",
    "\n",
    "            if (page_no!=0):\n",
    "                url=\"https://openlibrary.org\"+data['href'][:len(data['href'])-1]+str(page_no)\n",
    "                print(\"zzzzz Sleeping for \",delayTime,\" sec(s), from Robot.txt\")\n",
    "                time.sleep(delayTime)\n",
    "                print(\"ooooo Awake Now\")\n",
    "                \n",
    "                getLinksOfPageURL(url,page_no,urls,int(page_no),delayTime)\n",
    "\n",
    "        else:\n",
    "            print(f\"NOTE: Lister's Index of {index} has ZERO elements!\")\n",
    "            return []\n",
    "    \n",
    "    else:\n",
    "        delayTime = getRobotDelay(num_runs,delayTime)\n",
    "        page = requests.get(lister)\n",
    "        \n",
    "        print(\"Total Pages So Far: \", index)\n",
    "        page_no=index\n",
    "\n",
    "        try:\n",
    "            page = requests.get(lister)\n",
    "            soup = BeautifulSoup(page.content, \"html.parser\")\n",
    "            job_elements = soup.find(class_=\"pagination\")\n",
    "\n",
    "            pages= job_elements.find_all(class_=\"ChoosePage\",href=True)\n",
    "\n",
    "            print(f\"\\nFrom Page {index}...\")\n",
    "            for indexPage,data in enumerate(pages):\n",
    "                    if (\"Next\" in data.text or \"First\" in data.text or \"Previous\" in data.text):\n",
    "                        pass\n",
    "                    elif (int(data.text)>page_no):\n",
    "                        url=\"https://openlibrary.org\"+data['href']\n",
    "                        print (\"Found URL:\",url )\n",
    "                        urls.append(url)\n",
    "                        print(len(urls))\n",
    "                        page_no=int(data.text)\n",
    "\n",
    "            print(\"zzzzz Sleeping for \",delayTime,\" sec(s), from Robot.txt\")\n",
    "            time.sleep(delayTime)\n",
    "            print(\"ooooo Awake Now\")\n",
    "            \n",
    "            getLinksOfPageURL(url,page_no,urls,int(page_no),delayTime)\n",
    "        except:\n",
    "            print(\"End Of the Line.... No More Next Page\")\n",
    "            print(urls)\n",
    "            return urls\n",
    "        return urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ad72e628",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://openlibrary.org/search?q=paper+trains&mode=everything'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "formatSearchTerm4url(lister,0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11456435",
   "metadata": {},
   "source": [
    "# Get Each Page's URL and store in CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a44b618",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\bs4\\__init__.py:435: MarkupResemblesLocatorWarning: The input looks more like a filename than markup. You may want to open this file and pass the filehandle into Beautiful Soup.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "just got robot.txt... current delay time:  0.5\n",
      "NOTE: Lister's Index of 0 has ZERO elements!\n",
      "TOTAL LENGTH OF URLS COLLECTED for keywords' () ' : 0\n",
      "just got robot.txt... current delay time:  0.5\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "URL of Index 1...\n",
      "Found URL: https://openlibrary.org/search?q=paper+trains&mode=everything&page=1\n",
      "Found URL: https://openlibrary.org/search?q=paper+trains&mode=everything&page=2\n",
      "Found URL: https://openlibrary.org/search?q=paper+trains&mode=everything&page=3\n",
      "3\n",
      "Found URL: https://openlibrary.org/search?q=paper+trains&mode=everything&page=4\n",
      "4\n",
      "Found URL: https://openlibrary.org/search?q=paper+trains&mode=everything&page=5\n",
      "5\n",
      "Found URL: https://openlibrary.org/search?q=paper+trains&mode=everything&page=6\n",
      "6\n",
      "Found URL: https://openlibrary.org/search?q=paper+trains&mode=everything&page=7\n",
      "7\n",
      "Found URL: https://openlibrary.org/search?q=paper+trains&mode=everything&page=8\n",
      "8\n",
      "Found URL: https://openlibrary.org/search?q=paper+trains&mode=everything&page=9\n",
      "9\n",
      "Found URL: https://openlibrary.org/search?q=paper+trains&mode=everything&page=10\n",
      "10\n",
      "Found URL: https://openlibrary.org/search?q=paper+trains&mode=everything&page=2\n",
      "zzzzz Sleeping for  0.5  sec(s), from Robot.txt\n",
      "ooooo Awake Now\n",
      "Total Pages So Far:  10\n",
      "\n",
      "From Page 10...\n",
      "Found URL: https://openlibrary.org/search?q=paper+trains&page=11&mode=everything\n",
      "11\n",
      "Found URL: https://openlibrary.org/search?q=paper+trains&page=12&mode=everything\n",
      "12\n",
      "Found URL: https://openlibrary.org/search?q=paper+trains&page=13&mode=everything\n",
      "13\n",
      "Found URL: https://openlibrary.org/search?q=paper+trains&page=14&mode=everything\n",
      "14\n",
      "Found URL: https://openlibrary.org/search?q=paper+trains&page=15&mode=everything\n",
      "15\n",
      "zzzzz Sleeping for  0.5  sec(s), from Robot.txt\n",
      "ooooo Awake Now\n",
      "Total Pages So Far:  15\n",
      "\n",
      "From Page 15...\n",
      "Found URL: https://openlibrary.org/search?q=paper+trains&mode=everything&page=16\n",
      "16\n",
      "Found URL: https://openlibrary.org/search?q=paper+trains&mode=everything&page=17\n",
      "17\n",
      "Found URL: https://openlibrary.org/search?q=paper+trains&mode=everything&page=18\n",
      "18\n",
      "Found URL: https://openlibrary.org/search?q=paper+trains&mode=everything&page=19\n",
      "19\n",
      "Found URL: https://openlibrary.org/search?q=paper+trains&mode=everything&page=20\n",
      "20\n",
      "zzzzz Sleeping for  0.5  sec(s), from Robot.txt\n",
      "ooooo Awake Now\n",
      "just got robot.txt... current delay time:  0.5\n",
      "just got robot.txt... current delay time:  0.5\n",
      "Total Pages So Far:  20\n",
      "\n",
      "From Page 20...\n",
      "Found URL: https://openlibrary.org/search?q=paper+trains&mode=everything&page=21\n",
      "21\n",
      "Found URL: https://openlibrary.org/search?q=paper+trains&mode=everything&page=22\n",
      "22\n",
      "Found URL: https://openlibrary.org/search?q=paper+trains&mode=everything&page=23\n",
      "23\n",
      "Found URL: https://openlibrary.org/search?q=paper+trains&mode=everything&page=24\n",
      "24\n",
      "Found URL: https://openlibrary.org/search?q=paper+trains&mode=everything&page=25\n",
      "25\n",
      "zzzzz Sleeping for  0.5  sec(s), from Robot.txt\n",
      "ooooo Awake Now\n",
      "Total Pages So Far:  25\n",
      "\n",
      "From Page 25...\n",
      "Found URL: https://openlibrary.org/search?q=paper+trains&page=26&mode=everything\n",
      "26\n",
      "Found URL: https://openlibrary.org/search?q=paper+trains&page=27&mode=everything\n",
      "27\n",
      "Found URL: https://openlibrary.org/search?q=paper+trains&page=28&mode=everything\n",
      "28\n",
      "Found URL: https://openlibrary.org/search?q=paper+trains&page=29&mode=everything\n",
      "29\n",
      "Found URL: https://openlibrary.org/search?q=paper+trains&page=30&mode=everything\n",
      "30\n",
      "zzzzz Sleeping for  0.5  sec(s), from Robot.txt\n",
      "ooooo Awake Now\n",
      "Total Pages So Far:  30\n",
      "\n",
      "From Page 30...\n",
      "Found URL: https://openlibrary.org/search?q=paper+trains&mode=everything&page=31\n",
      "31\n",
      "Found URL: https://openlibrary.org/search?q=paper+trains&mode=everything&page=32\n",
      "32\n",
      "Found URL: https://openlibrary.org/search?q=paper+trains&mode=everything&page=33\n",
      "33\n",
      "Found URL: https://openlibrary.org/search?q=paper+trains&mode=everything&page=34\n",
      "34\n",
      "Found URL: https://openlibrary.org/search?q=paper+trains&mode=everything&page=35\n",
      "35\n",
      "zzzzz Sleeping for  0.5  sec(s), from Robot.txt\n",
      "ooooo Awake Now\n",
      "Total Pages So Far:  35\n",
      "\n",
      "From Page 35...\n",
      "Found URL: https://openlibrary.org/search?q=paper+trains&mode=everything&page=36\n",
      "36\n",
      "Found URL: https://openlibrary.org/search?q=paper+trains&mode=everything&page=37\n",
      "37\n",
      "Found URL: https://openlibrary.org/search?q=paper+trains&mode=everything&page=38\n",
      "38\n",
      "Found URL: https://openlibrary.org/search?q=paper+trains&mode=everything&page=39\n",
      "39\n",
      "Found URL: https://openlibrary.org/search?q=paper+trains&mode=everything&page=40\n",
      "40\n",
      "zzzzz Sleeping for  0.5  sec(s), from Robot.txt\n",
      "ooooo Awake Now\n",
      "just got robot.txt... current delay time:  0.5\n",
      "just got robot.txt... current delay time:  0.5\n",
      "Total Pages So Far:  40\n",
      "\n",
      "From Page 40...\n",
      "Found URL: https://openlibrary.org/search?q=paper+trains&mode=everything&page=41\n",
      "41\n",
      "Found URL: https://openlibrary.org/search?q=paper+trains&mode=everything&page=42\n",
      "42\n",
      "Found URL: https://openlibrary.org/search?q=paper+trains&mode=everything&page=43\n",
      "43\n",
      "Found URL: https://openlibrary.org/search?q=paper+trains&mode=everything&page=44\n",
      "44\n",
      "Found URL: https://openlibrary.org/search?q=paper+trains&mode=everything&page=45\n",
      "45\n",
      "zzzzz Sleeping for  0.5  sec(s), from Robot.txt\n",
      "ooooo Awake Now\n",
      "Total Pages So Far:  45\n",
      "\n",
      "From Page 45...\n",
      "Found URL: https://openlibrary.org/search?q=paper+trains&mode=everything&page=46\n",
      "46\n",
      "Found URL: https://openlibrary.org/search?q=paper+trains&mode=everything&page=47\n",
      "47\n",
      "Found URL: https://openlibrary.org/search?q=paper+trains&mode=everything&page=48\n",
      "48\n",
      "Found URL: https://openlibrary.org/search?q=paper+trains&mode=everything&page=49\n",
      "49\n",
      "Found URL: https://openlibrary.org/search?q=paper+trains&mode=everything&page=50\n",
      "50\n",
      "zzzzz Sleeping for  0.5  sec(s), from Robot.txt\n",
      "ooooo Awake Now\n",
      "Total Pages So Far:  50\n",
      "\n",
      "From Page 50...\n",
      "Found URL: https://openlibrary.org/search?q=paper+trains&mode=everything&page=51\n",
      "51\n",
      "Found URL: https://openlibrary.org/search?q=paper+trains&mode=everything&page=52\n",
      "52\n",
      "Found URL: https://openlibrary.org/search?q=paper+trains&mode=everything&page=53\n",
      "53\n",
      "Found URL: https://openlibrary.org/search?q=paper+trains&mode=everything&page=54\n",
      "54\n",
      "Found URL: https://openlibrary.org/search?q=paper+trains&mode=everything&page=55\n",
      "55\n",
      "zzzzz Sleeping for  0.5  sec(s), from Robot.txt\n",
      "ooooo Awake Now\n",
      "Total Pages So Far:  55\n",
      "\n",
      "From Page 55...\n",
      "Found URL: https://openlibrary.org/search?q=paper+trains&mode=everything&page=56\n",
      "56\n",
      "Found URL: https://openlibrary.org/search?q=paper+trains&mode=everything&page=57\n",
      "57\n",
      "Found URL: https://openlibrary.org/search?q=paper+trains&mode=everything&page=58\n",
      "58\n",
      "Found URL: https://openlibrary.org/search?q=paper+trains&mode=everything&page=59\n",
      "59\n",
      "Found URL: https://openlibrary.org/search?q=paper+trains&mode=everything&page=60\n",
      "60\n",
      "zzzzz Sleeping for  0.5  sec(s), from Robot.txt\n",
      "ooooo Awake Now\n",
      "just got robot.txt... current delay time:  0.5\n",
      "just got robot.txt... current delay time:  0.5\n",
      "Total Pages So Far:  60\n",
      "\n",
      "From Page 60...\n",
      "Found URL: https://openlibrary.org/search?q=paper+trains&mode=everything&page=61\n",
      "61\n",
      "Found URL: https://openlibrary.org/search?q=paper+trains&mode=everything&page=62\n",
      "62\n",
      "Found URL: https://openlibrary.org/search?q=paper+trains&mode=everything&page=63\n",
      "63\n",
      "Found URL: https://openlibrary.org/search?q=paper+trains&mode=everything&page=64\n",
      "64\n",
      "Found URL: https://openlibrary.org/search?q=paper+trains&mode=everything&page=65\n",
      "65\n",
      "zzzzz Sleeping for  0.5  sec(s), from Robot.txt\n",
      "ooooo Awake Now\n",
      "Total Pages So Far:  65\n",
      "\n",
      "From Page 65...\n",
      "Found URL: https://openlibrary.org/search?q=paper+trains&page=66&mode=everything\n",
      "66\n",
      "Found URL: https://openlibrary.org/search?q=paper+trains&page=67&mode=everything\n",
      "67\n",
      "Found URL: https://openlibrary.org/search?q=paper+trains&page=68&mode=everything\n",
      "68\n",
      "Found URL: https://openlibrary.org/search?q=paper+trains&page=69&mode=everything\n",
      "69\n",
      "Found URL: https://openlibrary.org/search?q=paper+trains&page=70&mode=everything\n",
      "70\n",
      "zzzzz Sleeping for  0.5  sec(s), from Robot.txt\n",
      "ooooo Awake Now\n",
      "Total Pages So Far:  70\n",
      "\n",
      "From Page 70...\n",
      "Found URL: https://openlibrary.org/search?q=paper+trains&page=71&mode=everything\n",
      "71\n",
      "Found URL: https://openlibrary.org/search?q=paper+trains&page=72&mode=everything\n",
      "72\n",
      "Found URL: https://openlibrary.org/search?q=paper+trains&page=73&mode=everything\n",
      "73\n",
      "Found URL: https://openlibrary.org/search?q=paper+trains&page=74&mode=everything\n",
      "74\n",
      "Found URL: https://openlibrary.org/search?q=paper+trains&page=75&mode=everything\n",
      "75\n",
      "zzzzz Sleeping for  0.5  sec(s), from Robot.txt\n"
     ]
    }
   ],
   "source": [
    "urlFile=[]\n",
    "for i in range(0,len(lister)):\n",
    "    #start with number of runs 0\n",
    "    urlFile.append(getLinksOfPageURL(lister,i,urlFile,i,0))\n",
    "    urlFile = [x for x in urlFile if x != []]\n",
    "    print(\"TOTAL LENGTH OF URLS COLLECTED for keywords\\'\",lister[i],\"\\' :\",len(urlFile))\n",
    "    \n",
    "    a = numpy.asarray(urlFile)\n",
    "    numpy.savetxt(f\"URLScrapedKeyWordsLinks.csv\", a, delimiter=\",\", fmt='%s')\n",
    "#     numpy.savetxt(f\"URLScrapedKeyWords_WITH_{len(urlFile)}_links.csv\", a, delimiter=\",\", fmt='%s')\n",
    "\n",
    "        \n",
    "#NOTE: total 2865 pages"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07d80a65",
   "metadata": {},
   "source": [
    "# Get Each Page's URL from CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "3efe22f3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column names are\n",
      "https://openlibrary.org/search?q=paper+trains&mode=everything&page=1\n",
      "https://openlibrary.org/search?q=paper+trains&mode=everything&page=2\n",
      "https://openlibrary.org/search?q=paper+trains&mode=everything&page=3\n",
      "https://openlibrary.org/search?q=paper+trains&mode=everything&page=4\n",
      "https://openlibrary.org/search?q=paper+trains&mode=everything&page=5\n",
      "https://openlibrary.org/search?q=paper+trains&mode=everything&page=6\n",
      "https://openlibrary.org/search?q=paper+trains&mode=everything&page=7\n",
      "https://openlibrary.org/search?q=paper+trains&mode=everything&page=8\n",
      "https://openlibrary.org/search?q=paper+trains&mode=everything&page=9\n",
      "https://openlibrary.org/search?q=paper+trains&mode=everything&page=10\n",
      "https://openlibrary.org/search?q=paper+trains&page=11&mode=everything\n",
      "https://openlibrary.org/search?q=paper+trains&page=12&mode=everything\n",
      "https://openlibrary.org/search?q=paper+trains&page=13&mode=everything\n",
      "https://openlibrary.org/search?q=paper+trains&page=14&mode=everything\n",
      "https://openlibrary.org/search?q=paper+trains&page=15&mode=everything\n",
      "https://openlibrary.org/search?q=paper+trains&mode=everything&page=16\n",
      "https://openlibrary.org/search?q=paper+trains&mode=everything&page=17\n",
      "https://openlibrary.org/search?q=paper+trains&mode=everything&page=18\n",
      "https://openlibrary.org/search?q=paper+trains&mode=everything&page=19\n",
      "https://openlibrary.org/search?q=paper+trains&mode=everything&page=20\n",
      "https://openlibrary.org/search?q=paper+trains&mode=everything&page=21\n",
      "https://openlibrary.org/search?q=paper+trains&mode=everything&page=22\n",
      "https://openlibrary.org/search?q=paper+trains&mode=everything&page=23\n",
      "https://openlibrary.org/search?q=paper+trains&mode=everything&page=24\n",
      "https://openlibrary.org/search?q=paper+trains&mode=everything&page=25\n",
      "https://openlibrary.org/search?q=paper+trains&page=26&mode=everything\n",
      "https://openlibrary.org/search?q=paper+trains&page=27&mode=everything\n",
      "https://openlibrary.org/search?q=paper+trains&page=28&mode=everything\n",
      "https://openlibrary.org/search?q=paper+trains&page=29&mode=everything\n",
      "https://openlibrary.org/search?q=paper+trains&page=30&mode=everything\n",
      "https://openlibrary.org/search?q=paper+trains&mode=everything&page=31\n",
      "https://openlibrary.org/search?q=paper+trains&mode=everything&page=32\n",
      "https://openlibrary.org/search?q=paper+trains&mode=everything&page=33\n",
      "https://openlibrary.org/search?q=paper+trains&mode=everything&page=34\n",
      "https://openlibrary.org/search?q=paper+trains&mode=everything&page=35\n",
      "https://openlibrary.org/search?q=paper+trains&mode=everything&page=36\n",
      "https://openlibrary.org/search?q=paper+trains&mode=everything&page=37\n",
      "https://openlibrary.org/search?q=paper+trains&mode=everything&page=38\n",
      "https://openlibrary.org/search?q=paper+trains&mode=everything&page=39\n",
      "https://openlibrary.org/search?q=paper+trains&mode=everything&page=40\n",
      "https://openlibrary.org/search?q=paper+trains&mode=everything&page=41\n",
      "https://openlibrary.org/search?q=paper+trains&mode=everything&page=42\n",
      "https://openlibrary.org/search?q=paper+trains&mode=everything&page=43\n",
      "https://openlibrary.org/search?q=paper+trains&mode=everything&page=44\n",
      "https://openlibrary.org/search?q=paper+trains&mode=everything&page=45\n",
      "https://openlibrary.org/search?q=paper+trains&mode=everything&page=46\n",
      "https://openlibrary.org/search?q=paper+trains&mode=everything&page=47\n",
      "https://openlibrary.org/search?q=paper+trains&mode=everything&page=48\n",
      "https://openlibrary.org/search?q=paper+trains&mode=everything&page=49\n",
      "https://openlibrary.org/search?q=paper+trains&mode=everything&page=50\n",
      "https://openlibrary.org/search?q=paper+trains&mode=everything&page=51\n",
      "https://openlibrary.org/search?q=paper+trains&mode=everything&page=52\n",
      "https://openlibrary.org/search?q=paper+trains&mode=everything&page=53\n",
      "https://openlibrary.org/search?q=paper+trains&mode=everything&page=54\n",
      "https://openlibrary.org/search?q=paper+trains&mode=everything&page=55\n",
      "https://openlibrary.org/search?q=paper+trains&mode=everything&page=56\n",
      "https://openlibrary.org/search?q=paper+trains&mode=everything&page=57\n",
      "https://openlibrary.org/search?q=paper+trains&mode=everything&page=58\n",
      "https://openlibrary.org/search?q=paper+trains&mode=everything&page=59\n",
      "https://openlibrary.org/search?q=paper+trains&mode=everything&page=60\n",
      "https://openlibrary.org/search?q=paper+trains&mode=everything&page=61\n",
      "https://openlibrary.org/search?q=paper+trains&mode=everything&page=62\n",
      "https://openlibrary.org/search?q=paper+trains&mode=everything&page=63\n",
      "https://openlibrary.org/search?q=paper+trains&mode=everything&page=64\n",
      "https://openlibrary.org/search?q=paper+trains&mode=everything&page=65\n",
      "https://openlibrary.org/search?q=paper+trains&page=66&mode=everything\n",
      "https://openlibrary.org/search?q=paper+trains&page=67&mode=everything\n",
      "https://openlibrary.org/search?q=paper+trains&page=68&mode=everything\n",
      "https://openlibrary.org/search?q=paper+trains&page=69&mode=everything\n",
      "https://openlibrary.org/search?q=paper+trains&page=70&mode=everything\n",
      "https://openlibrary.org/search?q=paper+trains&page=71&mode=everything\n",
      "https://openlibrary.org/search?q=paper+trains&page=72&mode=everything\n",
      "https://openlibrary.org/search?q=paper+trains&page=73&mode=everything\n",
      "https://openlibrary.org/search?q=paper+trains&page=74&mode=everything\n",
      "https://openlibrary.org/search?q=paper+trains&page=75&mode=everything\n",
      "None\n",
      "Processed 76 lines.\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "pageURLs = []\n",
    "with open('URLScrapedKeyWordsLinks.csv') as csv_file:\n",
    "    csv_reader = csv.reader(csv_file, delimiter=',')\n",
    "    line_count = 0\n",
    "    for row in csv_reader:\n",
    "        if line_count == 0:\n",
    "            print(f'Column names are\\n{\", \".join(row)}')\n",
    "            pageURLs.append(row)\n",
    "            line_count += 1\n",
    "        else:\n",
    "            print(f'{row[0]}')\n",
    "            line_count += 1\n",
    "            pageURLs.append(row)\n",
    "    print(f'Processed {line_count} lines.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be934b98",
   "metadata": {},
   "source": [
    "# Get Each Book's URL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "df140fa1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "just got robot.txt... current delay time:  0.5\n",
      "\n",
      "\n",
      "zzzzz Sleeping for  0.5  sec(s), from Robot.txt\n",
      "ooooo Awake Now\n",
      "https://openlibrary.org/works/OL6729940W\n",
      "https://openlibrary.org/works/OL6729938W\n",
      "https://openlibrary.org/works/OL6729935W\n",
      "https://openlibrary.org/works/OL6729930W\n",
      "https://openlibrary.org/works/OL6729928W\n",
      "https://openlibrary.org/works/OL6729927W\n",
      "https://openlibrary.org/works/OL6729909W\n",
      "https://openlibrary.org/works/OL25294299W\n",
      "https://openlibrary.org/works/OL24162515W?edition=ia%3Apapertrainechoes0000phei\n",
      "https://openlibrary.org/works/OL6729934W\n",
      "https://openlibrary.org/works/OL6729921W\n",
      "https://openlibrary.org/works/OL6729933W\n",
      "https://openlibrary.org/works/OL6729924W\n",
      "https://openlibrary.org/works/OL18883167W\n",
      "https://openlibrary.org/works/OL11929301W\n",
      "https://openlibrary.org/works/OL18883166W\n",
      "https://openlibrary.org/works/OL25547462W\n",
      "https://openlibrary.org/works/OL6729929W\n",
      "https://openlibrary.org/works/OL6729910W\n",
      "https://openlibrary.org/works/OL6729916W\n",
      "\n",
      "\n",
      "zzzzz Sleeping for  0.5  sec(s), from Robot.txt\n",
      "ooooo Awake Now\n",
      "https://openlibrary.org/works/OL6729931W\n",
      "https://openlibrary.org/works/OL6729915W\n",
      "https://openlibrary.org/works/OL6729903W\n",
      "https://openlibrary.org/works/OL6729923W\n",
      "https://openlibrary.org/works/OL6729904W\n",
      "https://openlibrary.org/works/OL6729926W\n",
      "https://openlibrary.org/works/OL6729906W\n",
      "https://openlibrary.org/works/OL15958283W\n",
      "https://openlibrary.org/works/OL15961076W\n",
      "https://openlibrary.org/works/OL16187404W\n",
      "https://openlibrary.org/works/OL6729932W\n",
      "https://openlibrary.org/works/OL8460962W\n",
      "https://openlibrary.org/works/OL6729939W\n",
      "https://openlibrary.org/works/OL12008994M\n",
      "https://openlibrary.org/works/OL12415439W\n",
      "https://openlibrary.org/works/OL13532870W\n",
      "https://openlibrary.org/works/OL7930532W\n",
      "https://openlibrary.org/works/OL18889672W\n",
      "https://openlibrary.org/works/OL1869308W\n",
      "https://openlibrary.org/works/OL11598848W\n",
      "\n",
      "\n",
      "zzzzz Sleeping for  0.5  sec(s), from Robot.txt\n",
      "ooooo Awake Now\n",
      "https://openlibrary.org/works/OL17787125W?edition=ia%3Atwopapersonmanua00harr\n",
      "https://openlibrary.org/works/OL6563375W\n",
      "https://openlibrary.org/works/OL18889671W\n",
      "https://openlibrary.org/works/OL13300606W\n",
      "https://openlibrary.org/works/OL19044930W\n",
      "https://openlibrary.org/works/OL28324595W\n",
      "https://openlibrary.org/works/OL13575674W\n",
      "https://openlibrary.org/works/OL9612478W?edition=ia%3Ayorkshiresurname0000heyd\n",
      "https://openlibrary.org/works/OL7932668W\n",
      "https://openlibrary.org/works/OL28340260W\n",
      "https://openlibrary.org/works/OL13532858W\n",
      "https://openlibrary.org/works/OL13532841W\n",
      "https://openlibrary.org/works/OL13147414W?edition=ia%3Atrainingmemorya00coopgoog\n",
      "https://openlibrary.org/works/OL17089463W?edition=ia%3Ab21358497\n",
      "https://openlibrary.org/works/OL20208645W\n",
      "https://openlibrary.org/works/OL8999128W\n",
      "https://openlibrary.org/works/OL12100425M\n",
      "https://openlibrary.org/works/OL9691071W?edition=ia%3Atrainingturnstoe0000ainl\n",
      "https://openlibrary.org/works/OL6529877W\n",
      "https://openlibrary.org/works/OL17228791M\n"
     ]
    }
   ],
   "source": [
    "delayTime = 0\n",
    "bookURLs =[]\n",
    "for index, item in enumerate(pageURLs):\n",
    "    delayTime = getRobotDelay(index,delayTime)\n",
    "    print(\"\\n\\nzzzzz Sleeping for \",delayTime,\" sec(s), from Robot.txt\")\n",
    "    time.sleep(delayTime)\n",
    "    print(\"ooooo Awake Now\")\n",
    "            \n",
    "    page = requests.get(pageURLs[index][0]) #url\n",
    "    soup = BeautifulSoup(page.content, \"html.parser\")\n",
    "    results = soup.find('ul', class_='list-books')\n",
    "    for item in results:\n",
    "        if (item.find(\"h3\") == -1):\n",
    "            pass\n",
    "        else:\n",
    "            url = \"https://openlibrary.org\"+item.find(\"a\")['href']\n",
    "            print(url)\n",
    "            bookURLs.append(url)\n",
    "    if (index ==2):\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 426,
   "id": "7db6fe70",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getBookDetails(soup):\n",
    "    dict1={}\n",
    "    \n",
    "    try:\n",
    "        results = soup.find('div', class_='workDetails')\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    #get Book Title\n",
    "    try:\n",
    "        dict1['title'] = results.find(\"h1\",class_=\"work-title\").text\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    #get Author of Book URL\n",
    "    try:\n",
    "        dict1[\"authorURL\"] = \"https://openlibrary.org\"+results.find(\"h2\",class_=\"edition-byline\").find(\"a\")['href']\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    #get Book Date\n",
    "    try:\n",
    "        dict1[\"date\"] = results.find(\"div\",  class_=\"smallest\").text.split(\" | \")[0]\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    #Get Subject(s) Name, URL\n",
    "    try:\n",
    "        dictNest={}\n",
    "        subjects = results.find(\"div\",  class_=\"subjects-content\").find(\"span\",class_=\"clamp\").find_all(\"a\")\n",
    "        for index, subject in enumerate(subjects):\n",
    "            dictNest[index] = {\"name\":subject.text, \"href\": \"https://openlibrary.org\"+subject['href']}\n",
    "        dict1[\"subject\"]=dictNest\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    #Get Published Date\n",
    "    publish_date = soup.find(\"div\",class_=\"edition-omniline-item\").find(\"span\").text\n",
    "    dict1[\"publish_date\"] = publish_date\n",
    "    \n",
    "    \n",
    "    #Book Details Section\n",
    "    sections = soup.find(\"div\",  class_=\"edition-info\").find_all(\"div\",class_=\"section\")\n",
    "    for section in sections:\n",
    "        try:\n",
    "            published_in = section.findChild(\"p\").text\n",
    "            dict1['published_in'] = published_in.replace(\"\\n          \",\"\")\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "            dewy = section.findChild(\"dl\")\n",
    "        try:\n",
    "\n",
    "\n",
    "            try:\n",
    "                for index,item in enumerate(dewy.find_all(\"dt\")):\n",
    "                    if (\"OCLC/WorldCat\" in dewy.find_all(\"dt\")[index].text):\n",
    "                        dict1[\"oclc_worldcat\"] = dewy.find_all(\"dd\")[index].text.replace(\"\\n\",\"\").replace(\" \",\"\")\n",
    "                    if (\"ISBN\" in dewy.find_all(\"dt\")[index].text):\n",
    "                        dict1[\"isbn\"] = dewy.find_all(\"dd\")[index].text.replace(\"\\n\",\"\").replace(\" \",\"\")\n",
    "                    if (\"Open Library\" in dewy.find_all(\"dt\")[index].text):\n",
    "                        dict1[\"open_library_id\"] = dewy.find_all(\"dd\")[index].text.replace(\"\\n\",\"\").replace(\" \",\"\")\n",
    "\n",
    "                    if (\"Dewey Decimal Class\" in dewy.find_all(\"dt\")[index].text):\n",
    "                        dict1[\"dewy_decimal_class\"] = dewy.findChild(\"dd\").text.replace(\"\\n\",\"\").replace(\" \",\"\")\n",
    "\n",
    "                    if  (\"Number of pages\" in dewy.find_all(\"dt\")[index].text):\n",
    "                        dict1[\"num_of_pages\"] = dewy.find_all(\"dd\")[index].text.replace(\"\\n\",\"\").replace(\" \",\"\").replace(\";\",\"\")\n",
    "                    if (\"Pagination\" in dewy.find_all(\"dt\")[index].text):\n",
    "                        dict1[\"pagination\"] = dewy.find_all(\"dd\")[index].text.replace(\" ;\\n\\n\",\"\")\n",
    "\n",
    "            except:\n",
    "                pass\n",
    "    \n",
    "        except:\n",
    "            pass\n",
    "    \n",
    "\n",
    "    \n",
    "    return dict1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 434,
   "id": "e5f56acd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "just got robot.txt... current delay time:  0.5\n",
      "\n",
      "\n",
      "zzzzz Sleeping for  0.5  sec(s), from Robot.txt\n",
      "ooooo Awake Now\n",
      "Saved CSV file as' OL6729940W.json'\n",
      "\n",
      "\n",
      "zzzzz Sleeping for  0.5  sec(s), from Robot.txt\n",
      "ooooo Awake Now\n",
      "Saved CSV file as' OL6729938W.json'\n",
      "\n",
      "\n",
      "zzzzz Sleeping for  0.5  sec(s), from Robot.txt\n",
      "ooooo Awake Now\n",
      "Saved CSV file as' OL6729935W.json'\n",
      "\n",
      "\n",
      "zzzzz Sleeping for  0.5  sec(s), from Robot.txt\n",
      "ooooo Awake Now\n",
      "Saved CSV file as' OL6729930W.json'\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "for index, item in enumerate(bookURLs):\n",
    "\n",
    "    delayTime = getRobotDelay(index,delayTime)\n",
    "    \n",
    "    print(\"\\n\\nzzzzz Sleeping for \",delayTime,\" sec(s), from Robot.txt\")\n",
    "    time.sleep(delayTime)\n",
    "    print(\"ooooo Awake Now\")\n",
    "    \n",
    "    page = requests.get(bookURLs[index]) #url\n",
    "    soup = BeautifulSoup(page.content, \"html.parser\")\n",
    "    \n",
    "    csvHere = getBookDetails(soup)\n",
    "    try:\n",
    "        with open(bookURLs[index].split(\"https://openlibrary.org/works/\")[1]+'.json', 'w') as outfile:\n",
    "            json.dump(csvHere, outfile)\n",
    "            \n",
    "        print(\"Saved CSV file as\\'\",bookURLs[index].split(\"https://openlibrary.org/works/\")[1]+'.json\\'')\n",
    "    except Exception as e:\n",
    "        print(\"Couldn't export to CSV for Book (URL)\"+bookURLs[index]+\"\\nStack Trace Here:\\n\")\n",
    "        print(e)\n",
    "        \n",
    "    #break after 4 saves\n",
    "    if (index == 3): break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b17d1544",
   "metadata": {},
   "source": [
    "# ------ Wiki.org (below)------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fb6b4da5",
   "metadata": {},
   "outputs": [],
   "source": [
    "words2Scrap=[\"Computer\", \"Glasgow\", \"United\", \"Kingdom\", \"Library\", \"Fog\", \"Empires\", \"Doctor\", \"Hospital\", \"Bachelor\", \n",
    "\"Degree\", \"Internet\", \"Things\", \"Information\", \"Info\", \"Retrieval\", \"Retrieve\", \"Info\", \"Universe\", \"University\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2f68fc7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import time\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from itertools import combinations\n",
    "import time\n",
    "\n",
    "#Source: https://stackoverflow.com/questions/464864/how-to-get-all-possible-combinations-of-a-list-s-elements#answer-32555776\n",
    "def combs(x):\n",
    "    return [c for i in range(len(x)+1) for c in combinations(x,i)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e7aefcba",
   "metadata": {},
   "outputs": [],
   "source": [
    "lister= list(combs(words2Scrap))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "504805c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1048576"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(lister)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5c0fab99",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Computer', 'Glasgow')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lister[21]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fe5848fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def formatSearchTerm4url(typle,index):\n",
    "    searchTerm=\"\"\n",
    "    for i,d in enumerate(list(typle[index])):\n",
    "        if (i!=0):\n",
    "            searchTerm+=\"+\"+str(d)\n",
    "        else:\n",
    "            searchTerm=d\n",
    "    urlMain= f\"https://en.wikipedia.org/w/index.php?search=\"+ searchTerm\n",
    "\n",
    "    # get the redirected URL's destination (.url) A.K.A non-Search query URL\n",
    "    urlMain = requests.get(urlMain).url\n",
    "    return urlMain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9b4c1aed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sleepingNow(delayTime):\n",
    "    print(\"\\n\\nzzzzz Sleeping for \",delayTime,\" sec(s)\")\n",
    "    time.sleep(delayTime)\n",
    "    print(\"ooooo Awake Now\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f89db069",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{3: {'asd': 1}}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "addedJson={}\n",
    "arrayz=[{\"asd1\":1},{\"asd2\":1},{\"asd3\":1}]\n",
    "addedJson[len(arrayz)]={\"asd\":1}\n",
    "addedJson"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "71c98907",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JH\n"
     ]
    }
   ],
   "source": [
    "found = True\n",
    "if (found!=False):\n",
    "    print(\"JH\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c5404180",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'asd': {'name': 1, 'gender': 2}, 'qwe': {'name': 1, 'gender': 2}}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "array1=[{\"asd\":{\"name\":1,\"gender\":2},\"qwe\":{\"name\":1,\"gender\":2}}]\n",
    "dict1={}\n",
    "for item in array1:\n",
    "    for key, value in item.items():\n",
    "        dict1[key]=value\n",
    "\n",
    "dict1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "51147c05",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Computer',)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lister[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a5cbe1ac",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Index:0\n",
      "Current Index:1\n",
      "Current Index:2\n",
      "Current Index:3\n",
      "Current Index:4\n",
      "Current Index:5\n",
      "Current Index:6\n",
      "Current Index:7\n",
      "Current Index:8\n",
      "Current Index:9\n",
      "Current Index:10\n",
      "Current Index:11\n",
      "Current Index:12\n",
      "Current Index:13\n",
      "Current Index:14\n",
      "Current Index:15\n",
      "Current Index:16\n",
      "Current Index:17\n",
      "Current Index:18\n",
      "Current Index:19\n",
      "Current Index:20\n",
      "Current Index:21\n",
      "\n",
      "\n",
      "zzzzz Sleeping for  5  sec(s)\n",
      "ooooo Awake Now\n",
      "[<a class=\"new\" href=\"/w/index.php?title=Computer_Glasgow&amp;action=edit&amp;redlink=1\" title=\"Computer Glasgow (page does not exist)\">Computer Glasgow</a>, <a href=\"/wiki/Wikipedia:Article_wizard\" title=\"Wikipedia:Article wizard\">create a draft and submit it for review</a>, <a data-serp-pos=\"0\" href=\"/wiki/University_of_Glasgow\" title=\"University of Glasgow\">University of <span class=\"searchmatch\">Glasgow</span></a>, <a data-serp-pos=\"1\" href=\"/wiki/Simon_Peyton_Jones\" title=\"Simon Peyton Jones\">Simon Peyton Jones</a>, <a data-serp-pos=\"2\" href=\"/wiki/The_Herald_(Glasgow)\" title=\"The Herald (Glasgow)\">The Herald (<span class=\"searchmatch\">Glasgow</span>)</a>, <a data-serp-pos=\"3\" href=\"/wiki/Glasgow_Caledonian_University\" title=\"Glasgow Caledonian University\"><span class=\"searchmatch\">Glasgow</span> Caledonian University</a>, <a data-serp-pos=\"4\" href=\"/wiki/Persistence_(computer_science)\" title=\"Persistence (computer science)\">Persistence (<span class=\"searchmatch\">computer</span> science)</a>, <a data-serp-pos=\"5\" href=\"/wiki/Gary_McKinnon\" title=\"Gary McKinnon\">Gary McKinnon</a>, <a data-serp-pos=\"6\" href=\"/wiki/Andrew_Colin\" title=\"Andrew Colin\">Andrew Colin</a>, <a data-serp-pos=\"7\" href=\"/wiki/Rector_of_the_University_of_Glasgow\" title=\"Rector of the University of Glasgow\">Rector of the University of <span class=\"searchmatch\">Glasgow</span></a>, <a data-serp-pos=\"8\" href=\"/wiki/Paul_Cockshott\" title=\"Paul Cockshott\">Paul Cockshott</a>, <a data-serp-pos=\"9\" href=\"/wiki/University_of_Strathclyde\" title=\"University of Strathclyde\">University of Strathclyde</a>, <a class=\"mw-redirect\" href=\"/wiki/Glasgow_and_West_of_Scotland_Technical_College\" title=\"Glasgow and West of Scotland Technical College\"><span class=\"searchmatch\">Glasgow</span> and West of Scotland Technical College</a>, <a data-serp-pos=\"10\" href=\"/wiki/C._J._van_Rijsbergen\" title=\"C. J. van Rijsbergen\">C. J. van Rijsbergen</a>, <a data-serp-pos=\"11\" href=\"/wiki/Deforestation_(computer_science)\" title=\"Deforestation (computer science)\">Deforestation (<span class=\"searchmatch\">computer</span> science)</a>, <a data-serp-pos=\"12\" href=\"/wiki/ABC\" title=\"ABC\">ABC</a>, <a data-serp-pos=\"13\" href=\"/wiki/Lauri_Love\" title=\"Lauri Love\">Lauri Love</a>, <a data-serp-pos=\"14\" href=\"/wiki/Computer-aided_assessment\" title=\"Computer-aided assessment\"><span class=\"searchmatch\">Computer</span>-aided assessment</a>, <a data-serp-pos=\"15\" href=\"/wiki/Collins_English_Dictionary\" title=\"Collins English Dictionary\">Collins English Dictionary</a>, <a data-serp-pos=\"16\" href=\"/wiki/Chemical_computer\" title=\"Chemical computer\">Chemical <span class=\"searchmatch\">computer</span></a>, <a data-serp-pos=\"17\" href=\"/wiki/Climate_change\" title=\"Climate change\">Climate change</a>, <a class=\"mw-redirect\" href=\"/wiki/Computer_models_of_global_warming\" title=\"Computer models of global warming\"><span class=\"searchmatch\">Computer</span> models of global warming</a>, <a data-serp-pos=\"18\" href=\"/wiki/Muffy_Calder\" title=\"Muffy Calder\">Muffy Calder</a>, <a data-serp-pos=\"19\" href=\"/wiki/Data_type\" title=\"Data type\">Data type</a>, <a class=\"mw-redirect\" href=\"/wiki/Type_(computer_science)\" title=\"Type (computer science)\">Type (<span class=\"searchmatch\">computer</span> science)</a>, <a class=\"extiw\" href=\"https://en.wikisource.org/wiki/Bioinformatics_Education%E2%80%94Perspectives_and_Challenges\" title=\"s:Bioinformatics EducationPerspectives and Challenges\">Bioinformatics EducationPerspectives and Challenges</a>, <a href=\"https://en.wikisource.org/wiki/Special:Search?search=Computer+Glasgow&amp;fulltext=1\" target=\"_blank\"> Texts from Wikisource</a>, <a class=\"extiw\" href=\"https://en.wikiquote.org/wiki/United_States_Marine_Corps\" title=\"q:United States Marine Corps\">United States Marine Corps</a>, <a href=\"https://en.wikiquote.org/wiki/Special:Search?search=Computer+Glasgow&amp;fulltext=1\" target=\"_blank\"> Quotes from Wikiquote</a>, <a class=\"extiw\" href=\"https://en.wikibooks.org/wiki/Instructional_Technology/Instructional_Design/Design_Models_for_Professional_Development\" title=\"b:Instructional Technology/Instructional Design/Design Models for Professional Development\">Instructional Technology/Instructional Design/Design Models for Professional Development</a>, <a href=\"https://en.wikibooks.org/wiki/Special:Search?search=Computer+Glasgow&amp;fulltext=1\" target=\"_blank\"> Textbooks from Wikibooks</a>, <a class=\"mw-nextlink\" href=\"/w/index.php?title=Special:Search&amp;limit=20&amp;offset=20&amp;profile=default&amp;search=Computer+Glasgow\" title=\"Next 20 results\">next 20</a>, <a class=\"mw-numlink\" href=\"/w/index.php?title=Special:Search&amp;limit=20&amp;offset=0&amp;profile=default&amp;search=Computer+Glasgow\" title=\"Show 20 results per page\">20</a>, <a class=\"mw-numlink\" href=\"/w/index.php?title=Special:Search&amp;limit=50&amp;offset=0&amp;profile=default&amp;search=Computer+Glasgow\" title=\"Show 50 results per page\">50</a>, <a class=\"mw-numlink\" href=\"/w/index.php?title=Special:Search&amp;limit=100&amp;offset=0&amp;profile=default&amp;search=Computer+Glasgow\" title=\"Show 100 results per page\">100</a>, <a class=\"mw-numlink\" href=\"/w/index.php?title=Special:Search&amp;limit=250&amp;offset=0&amp;profile=default&amp;search=Computer+Glasgow\" title=\"Show 250 results per page\">250</a>, <a class=\"mw-numlink\" href=\"/w/index.php?title=Special:Search&amp;limit=500&amp;offset=0&amp;profile=default&amp;search=Computer+Glasgow\" title=\"Show 500 results per page\">500</a>]\n"
     ]
    }
   ],
   "source": [
    "def getLinksFromSearch(lister,index,timeArray,wikiArray,depth):\n",
    "\n",
    "    for indexCurr,item in enumerate(lister):\n",
    "        print(\"Current Index:\"+str(indexCurr))\n",
    "        if (indexCurr <21): #== 0\n",
    "            pass\n",
    "        elif (indexCurr<index+1):\n",
    "            \n",
    "#             if(depth!=0):\n",
    "            addedJson={}\n",
    "            URL = formatSearchTerm4url(lister,indexCurr)\n",
    "\n",
    "\n",
    "            #if using Keyword Search\n",
    "            if(type(URL)==str):\n",
    "                sleepingNow(5)\n",
    "                page = requests.get(URL)\n",
    "\n",
    "                soup = BeautifulSoup(page.content, \"html.parser\")\n",
    "                job_elements = soup.find(\"h1\",{\"id\":\"firstHeading\"})\n",
    "#                 print(job_elements.prettify())\n",
    "                \n",
    "                if (\"Search results\" in job_elements.text):\n",
    "                    #returned page is a search result page with multiple wiki pages\n",
    "                    job_elements = soup.find(\"div\",{\"class\":\"searchresults\"}).find_all(\"a\",href=True)\n",
    "                    print(job_elements)\n",
    "                    break\n",
    "#                     pass\n",
    "#                 else:\n",
    "#                     contents = soup.find(\"div\",{\"id\":\"content\"})\n",
    "#                     contentSoup = contents.find(\"div\",{\"id\":\"mw-content-text\"}).find_all(href=True)\n",
    "#                     for index,item in enumerate(contentSoup):\n",
    "\n",
    "#                         #if href (URL) already well-formatted with HTTP(S)\n",
    "#                         if (\"http\" not in item[\"href\"]):\n",
    "#                             hrefCase = \"https://en.wikipedia.org\"+item[\"href\"]\n",
    "#                         elif ((\"wikipedia\" not in item[\"href\"]) or (\"title=Template\" in item[\"href\"]) or (\"&action=edit\" in item[\"href\"])):\n",
    "#                             pass #wiki not and title=Template/@action=edit\n",
    "#                         else:\n",
    "#                             hrefCase = item['href']\n",
    "\n",
    "#                         toCheck =[URL,hrefCase]\n",
    "#                         for url in toCheck:\n",
    "#                             if (\"http\" not in url):\n",
    "#                                 url = \"https://en.wikipedia.org\"+url\n",
    "#                             #if URL is a file with extension type '.' or not in dictionary\n",
    "#                             if ((\".\" not in url.split(\"/\")[-1]) and (url not in wikiArray)):\n",
    "\n",
    "#                                 #if referencing to a section in current wiki page, then ignore (not add to URL Frontier)\n",
    "#                                 if ((url[0] == \"#\") or (\"mw-data:TemplateStyles\" in hrefCase)):\n",
    "#                                     pass\n",
    "\n",
    "#                                 #if no redirect to same wiki page but different section\n",
    "#                                 #check if href base URL already in dictionary\n",
    "#                                 else:\n",
    "\n",
    "#                                     #if potential # in URL, investigate\n",
    "#                                     if (\"#\" in url):\n",
    "#                                         #if base url is not in addedJson, add into Dict\n",
    "#                                         if (url.split(\"#\")[0] not in addedJson):\n",
    "#                                             found = False\n",
    "#                                             #not found in wikiArray\n",
    "#                                             for index,item in enumerate(wikiArray):\n",
    "#                                                 if (url.split(\"#\")[0] in item):\n",
    "#                                                     found = True\n",
    "\n",
    "#                                             if (found==False):\n",
    "#                                                 addedJson[url.split(\"#\")[0]]={\"title\":\"\",\"content\":\"\"}\n",
    "#                                                 print(\"***Added URL:\",url.split(\"#\")[0])\n",
    "\n",
    "#                                         #if duplicated URL, don't add into Dict\n",
    "#                                         else:\n",
    "#                                             print(\"duplicated URL (not adding):   \",url)\n",
    "#                                             pass\n",
    "                                            \n",
    "\n",
    "#                                     else:\n",
    "#                                         #if base url is not in addedJson\n",
    "#                                         if (url not in addedJson):\n",
    "#                                             found = False\n",
    "\n",
    "#                                             #not found in wikiArray\n",
    "#                                             for index,item in enumerate(wikiArray):\n",
    "#                                                 if (url in item):\n",
    "#                                                     found = True\n",
    "\n",
    "#                                             #add into Dict\n",
    "#                                             if (found==False):\n",
    "#                                                 addedJson[url]={\"title\":\"\",\"content\":\"\"}\n",
    "#                                                 print(\"***Added URL:\",url)\n",
    "\n",
    "\n",
    "#                                         #if duplicated URL, don't add into Dict\n",
    "#                                         else:\n",
    "#                                             print(\"duplicated URL (not adding):   \",url)\n",
    "#                                             pass\n",
    "                                            \n",
    "                    wikiArray.append(addedJson)\n",
    "                    print(\"URL \"+str(indexCurr)+\" of \"+str(len(lister))+\"\\n\")\n",
    "                    print(\"Total No. of URLs at Depth No. \"+str(depth)+\": \",str(len(wikiArray[len(wikiArray)-1].keys())))\n",
    "                    \n",
    "\n",
    "        else:\n",
    "            \n",
    "            try:\n",
    "                wikiJson ={}\n",
    "                dict1={}\n",
    "\n",
    "                for item in wikiArray:\n",
    "                    for key, value in item.items():\n",
    "                        wikiJson[key]=value\n",
    "                dict1[\"all_content\"]=wikiJson\n",
    "                with open('wikiFile.json', 'w') as outfile:\n",
    "                    json.dump(dict1, outfile)\n",
    "            except Exception as e:\n",
    "                print(\"Couldn't export to CSV for Wiki URLs... Stack Trace Here:\\n\")\n",
    "                print(e)\n",
    "\n",
    "#start with empty dictionary {}\n",
    "newArray =[]\n",
    "getLinksFromSearch(lister,len(lister)-1,[10*60,time.time()], newArray,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c36759d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
